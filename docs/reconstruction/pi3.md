# Ï€Â³: Scalable Permutation-Equivariant Visual Geometry Learning (arXiv 2025)

![Pi3 Teaser](https://yyfz.github.io/pi3/assets/images/main.png)
*Ï€Â³ eliminates reference view bias through permutation-equivariant architecture, achieving state-of-the-art visual geometry reconstruction*

## ðŸ“‹ Overview
- **Authors**: Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, Tong He
- **Institution**: Shanghai AI Lab, Zhejiang University, SenseTime
- **Venue**: arXiv preprint (2025)
- **Links**: [Paper](https://arxiv.org/abs/2507.13347) | [Project Page](https://yyfz.github.io/pi3/) | [Code](https://github.com/yyfz/Pi3) | [Demo](https://huggingface.co/spaces/yyfz233/Pi3)
- **TL;DR**: Permutation-equivariant neural network that achieves state-of-the-art visual geometry reconstruction without requiring fixed reference views.

## ðŸŽ¯ Key Contributions

1. **Permutation-Equivariant Architecture**: First method to systematically eliminate reference view bias
2. **Order-Invariant Reconstruction**: Consistent results regardless of input image sequence
3. **Affine-Invariant Poses**: Direct prediction without fixed reference viewpoint
4. **SOTA Performance**: 55.7% better on Sintel, 57.4 FPS inference speed

## ðŸ”§ Technical Details

### Architecture Components

1. **Transformer Architecture**: Alternating view-wise and global self-attention blocks
2. **DINOv2 Backbone**: Embeds image views into patch tokens without positional encoding
3. **Permutation-Equivariant Processing**: Treats all views symmetrically without designated reference
4. **Dual Output**: Affine-invariant camera poses + scale-invariant local point maps

### Key Innovation: True Permutation Equivariance
- **Problem**: Previous methods rely on fixed reference views, causing ordering bias
- **Solution**: Symmetric processing of all input views
- **Result**: Near-zero variance across different input permutations

## ðŸ“Š Results

### Camera Pose Estimation (Zero-shot)
| Dataset | Fast3R | VGGT | **Ï€Â³** | Improvement |
|---------|--------|------|---------|-------------|
| Sintel ATE â†“ | 0.371 | 0.167 | **0.074** | 55.7% vs VGGT |
| TUM-dyn ATE â†“ | 0.090 | 0.012 | **0.014** | Competitive |
| RE10K RTA@30 â†‘ | 81.86 | 93.13 | **95.62** | +2.7% |

### Multi-view Stereo Reconstruction
| Dataset | Metric | VGGT | **Ï€Â³** | Improvement |
|---------|--------|------|---------|-------------|
| DTU | Accuracy â†“ | 1.338 | **1.198** | 10.5% better |
| DTU | Completeness â†“ | 1.896 | **1.849** | 2.5% better |
| ETH3D | Accuracy â†“ | 0.280 | **0.194** | 30.7% better |
| ETH3D | Completeness â†“ | 0.305 | **0.210** | 31.1% better |

### Performance Summary
- **Speed**: 57.4 FPS (vs VGGT 43.2 FPS, DUSt3R 1.25 FPS)
- **Order Robustness**: Near-zero variance across input permutations  
- **Video Depth**: 0.233 AbsRel (vs VGGT 0.299)
- **State-of-the-art**: Best results across camera pose, point maps, video depth

## ðŸ’¡ Critical Analysis

### Strengths
- **Reference View Elimination**: Solves fundamental bias problem in visual geometry
- **Order Invariance**: Robust to arbitrary input sequence permutations
- **Superior Performance**: State-of-the-art across multiple benchmarks
- **Faster Training**: Better convergence due to symmetric architecture

### Limitations
- **Computational Overhead**: Transformer architecture may be slower than simpler methods
- **Memory Requirements**: Processing all views symmetrically increases memory usage
- **Limited Scale Testing**: Evaluation mostly on standard benchmarks

### Applications
- **Multi-camera Systems**: Autonomous vehicles, surveillance, robotics
- **AR/VR**: Flexible reconstruction without view ordering constraints
- **Photogrammetry**: Professional 3D scanning and mapping

## ðŸ”— Method Comparison

| Method | Reference View | Order Dependence | Speed (FPS) | Performance |
|--------|----------------|------------------|-------------|-------------|
| DUSt3R | Required | High | 1.25 | Baseline |
| VGGT | Required | Medium | 43.2 | Good |
| **Ï€Â³** | **None** | **None** | **57.4** | **SOTA** |

### Key Differences from VGGT
- **Architecture**: Eliminates reference view bias through symmetric processing
- **Robustness**: Invariant to input ordering (near-zero variance)
- **Performance**: 55.7% better camera pose estimation on Sintel
- **Speed**: 33% faster inference (57.4 vs 43.2 FPS)

## ðŸ“š Conclusions

### Key Achievements
1. **Reference-Free Reconstruction**: First method to systematically eliminate reference view bias
2. **State-of-the-Art Performance**: Superior results across camera pose, point maps, and video depth
3. **Order Invariance**: Robust to arbitrary input permutations
4. **Practical Speed**: 57.4 FPS enables real-time applications

### Impact
- **Consumer**: More robust 3D capture from unordered photo collections
- **Professional**: Reliable multi-camera systems without view ordering constraints
- **Research**: Establishes permutation equivariance as key principle for visual geometry

### Significance

Ï€Â³ proves that eliminating reference view bias through permutation-equivariant design leads to both superior performance and increased robustness, establishing a new paradigm for visual geometry learning.