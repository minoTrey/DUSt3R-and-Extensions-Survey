# Ï€Â³: Scalable Permutation-Equivariant Visual Geometry Learning (arXiv 2025)

![Pi3 Teaser](https://yyfz.github.io/pi3/assets/images/main.png)
*Ï€Â³ achieves true permutation equivariance in visual geometry through symmetric architecture design*

## ðŸ“‹ Overview
- **Authors**: Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, Tong He
- **Institution**: Shanghai AI Lab, Zhejiang University, SenseTime
- **Venue**: arXiv preprint (2025)
- **Links**: [Paper](https://arxiv.org/abs/2507.13347) | [Project Page](https://yyfz.github.io/pi3/) | [Code](https://github.com/yyfz/Pi3) | [Demo](https://huggingface.co/spaces/yyfz233/Pi3)
- **TL;DR**: First visual geometry model achieving true permutation equivariance by eliminating all order-dependent components, surpassing VGGT across all benchmarks.

## ðŸŽ¯ Key Contributions

1. **True Permutation Equivariance**: Eliminates positional embeddings and reference frame bias completely
2. **Symmetric Architecture**: 36 alternating attention layers treating all views equally
3. **SOTA Performance**: Best results on camera pose, point maps, video depth, with near-zero order variance
4. **Efficiency**: 959M params outperforming models with 1.26B-5.57B params

## ðŸ”§ Technical Architecture

### Core Innovation: Eliminating Order Dependence

**Traditional approaches (including VGGT)**:
```python
tokens = patch_features + positional_embeddings  # Order-dependent!
# First frame gets special treatment as reference
```

**Ï€Â³'s approach**:
```python
tokens = DINOv2(images)  # Only content, no positions
# All frames processed identically
```

### Architecture Details

1. **Feature Extraction**: DINOv2 backbone (frozen)
   - Extracts patch tokens without positional information
   - Pure content-based representations

2. **36 Alternating Attention Layers**:
   - **Odd layers**: View-wise self-attention (within each image)
   - **Even layers**: Global self-attention (across all images)
   - No special treatment for any view

3. **Key Design Choices**:
   - **No positional embeddings**: Ensures order invariance
   - **No reference frame**: All views have equal status
   - **Symmetric processing**: f(Ï€(X)) = Ï€(f(X)) for any permutation Ï€

### Comparison with VGGT

| Component | VGGT | Ï€Â³ |
|-----------|------|-----|
| Layers | 24 alternating | 36 alternating |
| Reference frame | First frame special | All frames equal |
| Positional encoding | Yes | **No** |
| Order dependence | Partial | **None** |
| Parameters | 1.26B | 959M |

### Loss Function Design

Ï€Â³ uses a carefully designed composite loss function:

```
L_total = L_points + Î»_normalÂ·L_normal + Î»_confÂ·L_conf + Î»_camÂ·L_cam
```

**Key Loss Components**:
1. **Point Reconstruction Loss (L_points)**: Depth-weighted L1 distance for robust 3D point prediction
2. **Normal Loss (L_normal)**: Ensures consistent surface orientation across views
3. **Confidence Loss (L_conf)**: Binary cross-entropy for reliability estimation
4. **Camera Pose Loss (L_cam)**: Rotation and translation components with scale invariance

**Critical Design Choice**: Unlike methods that enforce permutation equivariance through loss terms, Ï€Â³ achieves it purely through architecture. The loss function focuses on reconstruction quality while the architecture guarantees f(Ï€(X)) = Ï€(f(X)).

## ðŸ“Š Results

### Table 1: Camera Pose Estimation on RealEstate10K and Co3Dv2
| Method | RealEstate10K (unseen) | | | Co3Dv2 (seen) | | |
|--------|------------------------|-------|-------|----------------|-------|-------|
| | RRA@30â†‘ | RTA@30â†‘ | AUC@30â†‘ | RRA@30â†‘ | RTA@30â†‘ | AUC@30â†‘ |
| Fast3R | 99.05 | 81.86 | 61.68 | 97.49 | 91.11 | 73.43 |
| CUT3R | 99.82 | 95.10 | 81.47 | 96.19 | 92.69 | 75.82 |
| FLARE | 99.69 | 95.23 | 80.01 | 96.38 | 93.76 | 73.99 |
| VGGT | 99.97 | 93.13 | 77.62 | 98.96 | 97.13 | 88.59 |
| **Ï€Â³** | **99.99** | **95.62** | **85.90** | **99.05** | **97.33** | **88.41** |

### Table 2: Camera Pose Estimation on Sintel, TUM-dynamics and ScanNet
| Method | Sintel (zero-shot) | | | TUM-dynamics (zero-shot) | | | ScanNet (seen) | | |
|--------|-------------------|---------|---------|-------------------------|---------|---------|----------------|---------|----------|
| | ATEâ†“ | RPE transâ†“ | RPE rotâ†“ | ATEâ†“ | RPE transâ†“ | RPE rotâ†“ | ATEâ†“ | RPE transâ†“ | RPE rotâ†“ |
| Fast3R | 0.371 | 0.298 | 13.75 | 0.090 | 0.101 | 1.425 | 0.155 | 0.123 | 3.491 |
| CUT3R | 0.217 | 0.070 | 0.636 | 0.047 | 0.015 | 0.451 | 0.094 | 0.022 | 0.629 |
| Aether | 0.189 | 0.054 | 0.694 | 0.092 | 0.012 | 1.106 | 0.176 | 0.028 | 1.204 |
| FLARE | 0.207 | 0.090 | 3.015 | 0.026 | 0.013 | 0.475 | 0.064 | 0.023 | 0.971 |
| VGGT | 0.167 | 0.062 | 0.491 | 0.012 | 0.010 | 0.311 | 0.035 | 0.015 | 0.382 |
| **Ï€Â³** | **0.074** | **0.040** | **0.282** | **0.014** | **0.009** | **0.312** | **0.031** | **0.013** | **0.347** |

### Table 3: Point Map Estimation on DTU and ETH3D
| Method | DTU | | | | | | ETH3D | | | | | |
|--------|-----|-----|-----|-----|-----|-----|-------|-----|-----|-----|-----|-----|
| | Acc.â†“ Mean | Med. | Comp.â†“ Mean | Med. | N.C.â†‘ Mean | Med. | Acc.â†“ Mean | Med. | Comp.â†“ Mean | Med. | N.C.â†‘ Mean | Med. |
| Fast3R | 3.340 | 1.919 | 2.929 | 1.125 | 0.671 | 0.755 | 0.832 | 0.691 | 0.978 | 0.683 | 0.667 | 0.766 |
| CUT3R | 4.742 | 2.600 | 3.400 | 1.316 | 0.679 | 0.764 | 0.617 | 0.525 | 0.747 | 0.579 | 0.754 | 0.848 |
| FLARE | 2.541 | 1.468 | 3.174 | 1.420 | 0.684 | 0.774 | 0.464 | 0.338 | 0.664 | 0.395 | 0.744 | 0.864 |
| VGGT | 1.338 | 0.779 | 1.896 | 0.992 | 0.676 | 0.766 | 0.280 | 0.185 | 0.305 | 0.182 | 0.853 | 0.950 |
| **Ï€Â³** | **1.198** | **0.646** | **1.849** | **0.607** | **0.678** | **0.768** | **0.194** | **0.131** | **0.210** | **0.128** | **0.883** | **0.969** |

### Table 4: Point Map Estimation on 7-Scenes and NRGBD
| Method | View | 7-Scenes | | | | | | NRGBD | | | | | |
|--------|------|----------|-----|-----|-----|-----|-----|-------|-----|-----|-----|-----|-----|
| | | Acc.â†“ Mean | Med. | Comp.â†“ Mean | Med. | NC.â†‘ Mean | Med. | Acc.â†“ Mean | Med. | Comp.â†“ Mean | Med. | NC.â†‘ Mean | Med. |
| Fast3R | sparse | 0.096 | 0.065 | 0.145 | 0.093 | 0.672 | 0.760 | 0.135 | 0.091 | 0.163 | 0.104 | 0.759 | 0.877 |
| CUT3R | | 0.094 | 0.051 | 0.101 | 0.050 | 0.703 | 0.804 | 0.104 | 0.041 | 0.079 | 0.031 | 0.822 | 0.968 |
| FLARE | | 0.085 | 0.058 | 0.142 | 0.104 | 0.695 | 0.779 | 0.053 | 0.024 | 0.051 | 0.025 | 0.877 | 0.988 |
| VGGT | | 0.046 | 0.026 | 0.057 | 0.034 | 0.728 | 0.842 | 0.051 | 0.029 | 0.066 | 0.038 | 0.890 | 0.981 |
| **Ï€Â³** | | **0.048** | **0.028** | **0.072** | **0.047** | **0.742** | **0.842** | **0.026** | **0.015** | **0.028** | **0.014** | **0.916** | **0.992** |
| Fast3R | dense | 0.038 | 0.015 | 0.056 | 0.018 | 0.645 | 0.725 | 0.072 | 0.030 | 0.050 | 0.016 | 0.790 | 0.934 |
| CUT3R | | 0.022 | 0.010 | 0.027 | 0.009 | 0.668 | 0.762 | 0.086 | 0.037 | 0.048 | 0.017 | 0.800 | 0.953 |
| FLARE | | 0.018 | 0.007 | 0.027 | 0.014 | 0.681 | 0.781 | 0.023 | 0.011 | 0.018 | 0.008 | 0.882 | 0.986 |
| VGGT | | 0.022 | 0.008 | 0.027 | 0.013 | 0.663 | 0.757 | 0.017 | 0.010 | 0.015 | 0.005 | 0.893 | 0.988 |
| **Ï€Â³** | | **0.015** | **0.007** | **0.022** | **0.011** | **0.687** | **0.790** | **0.015** | **0.008** | **0.013** | **0.005** | **0.898** | **0.987** |

### Table 5: Video Depth Estimation on Sintel, Bonn and KITTI
| Method | Params | Align | Sintel | | Bonn | | KITTI | | FPS |
|--------|--------|-------|--------|-------|------|-------|-------|-------|---------|
| | | | Abs Relâ†“ | Î´<1.25â†‘ | Abs Relâ†“ | Î´<1.25â†‘ | Abs Relâ†“ | Î´<1.25â†‘ | |
| DUSt3R | 571M | scale | 0.662 | 0.434 | 0.151 | 0.839 | 0.143 | 0.814 | 1.25 |
| MASt3R | 689M | | 0.558 | 0.487 | 0.188 | 0.765 | 0.115 | 0.848 | 1.01 |
| MonST3R | 571M | | 0.399 | 0.519 | 0.072 | 0.957 | 0.107 | 0.884 | 1.27 |
| Fast3R | 648M | | 0.638 | 0.422 | 0.194 | 0.772 | 0.138 | 0.834 | 65.8 |
| MVDUSt3R | 661M | | 0.805 | 0.283 | 0.426 | 0.357 | 0.456 | 0.342 | 0.69 |
| CUT3R | 793M | | 0.417 | 0.507 | 0.078 | 0.937 | 0.122 | 0.876 | 6.98 |
| Aether | 5.57B | | 0.324 | 0.502 | 0.273 | 0.594 | 0.056 | 0.978 | 6.14 |
| FLARE | 1.40B | | 0.729 | 0.336 | 0.152 | 0.790 | 0.356 | 0.570 | 1.75 |
| VGGT | 1.26B | | 0.299 | 0.638 | 0.057 | 0.966 | 0.062 | 0.969 | 43.2 |
| **Ï€Â³** | **959M** | | **0.233** | **0.664** | **0.049** | **0.975** | **0.038** | **0.986** | **57.4** |
| DUSt3R | 571M | scale&shift | 0.570 | 0.493 | 0.152 | 0.835 | 0.135 | 0.818 | 1.25 |
| MASt3R | 689M | | 0.480 | 0.517 | 0.189 | 0.771 | 0.115 | 0.849 | 1.01 |
| MonST3R | 571M | | 0.402 | 0.526 | 0.070 | 0.958 | 0.098 | 0.883 | 1.27 |
| Fast3R | 648M | | 0.518 | 0.486 | 0.196 | 0.768 | 0.139 | 0.808 | 65.8 |
| MVDUSt3R | 661M | | 0.619 | 0.332 | 0.482 | 0.357 | 0.401 | 0.355 | 0.69 |
| CUT3R | 793M | | 0.534 | 0.558 | 0.075 | 0.943 | 0.111 | 0.883 | 6.98 |
| Aether | 5.57B | | 0.314 | 0.604 | 0.308 | 0.602 | 0.054 | 0.977 | 6.14 |
| FLARE | 1.40B | | 0.791 | 0.358 | 0.142 | 0.797 | 0.357 | 0.579 | 1.75 |
| VGGT | 1.26B | | 0.230 | 0.678 | 0.052 | 0.969 | 0.052 | 0.968 | 43.2 |
| **Ï€Â³** | **959M** | | **0.210** | **0.726** | **0.043** | **0.975** | **0.037** | **0.985** | **57.4** |

### Table 6: Monocular Depth Estimation on Sintel, Bonn, KITTI and NYU-v2
| Method | Sintel | | Bonn | | KITTI | | NYU-v2 | |
|--------|--------|-------|------|-------|-------|-------|--------|-------|
| | Abs Relâ†“ | Î´<1.25â†‘ | Abs Relâ†“ | Î´<1.25â†‘ | Abs Relâ†“ | Î´<1.25â†‘ | Abs Relâ†“ | Î´<1.25â†‘ |
| DUSt3R | 0.488 | 0.532 | 0.139 | 0.832 | 0.109 | 0.873 | 0.081 | 0.909 |
| MASt3R | 0.413 | 0.569 | 0.123 | 0.833 | 0.077 | 0.948 | 0.110 | 0.865 |
| MonST3R | 0.402 | 0.525 | 0.069 | 0.954 | 0.098 | 0.895 | 0.094 | 0.887 |
| Fast3R | 0.544 | 0.509 | 0.169 | 0.796 | 0.120 | 0.861 | 0.093 | 0.898 |
| CUT3R | 0.418 | 0.520 | 0.058 | 0.967 | 0.097 | 0.914 | 0.081 | 0.914 |
| FLARE | 0.606 | 0.402 | 0.130 | 0.836 | 0.312 | 0.513 | 0.089 | 0.898 |
| VGGT | 0.335 | 0.599 | 0.053 | 0.970 | 0.082 | 0.947 | 0.056 | 0.951 |
| MoGe | 0.273 | 0.695 | 0.050 | 0.976 | 0.049 | 0.979 | 0.055 | 0.952 |
| - v1 | 0.273 | 0.695 | 0.050 | 0.976 | 0.054 | 0.977 | 0.055 | 0.952 |
| - v2 | 0.277 | 0.687 | 0.063 | 0.973 | 0.049 | 0.979 | 0.060 | 0.940 |
| **Ï€Â³** | **0.277** | **0.614** | **0.044** | **0.976** | **0.060** | **0.971** | **0.054** | **0.956** |

### Table 7: Standard Deviation of Point Cloud Estimation on DTU and ETH3D
| Method | DTU | | | | | | ETH3D | | | | | |
|--------|-----|-----|-----|-----|-----|-----|-------|-----|-----|-----|-----|-----|
| | Acc. std.â†“ Mean | Med. | Comp. std.â†“ Mean | Med. | N.C. std.â†“ Mean | Med. | Acc. std.â†“ Mean | Med. | Comp. std.â†“ Mean | Med. | N.C. std.â†“ Mean | Med. |
| Fast3R | 0.578 | 0.451 | 0.677 | 0.376 | 0.007 | 0.009 | 0.182 | 0.205 | 0.381 | 0.273 | 0.047 | 0.072 |
| CUT3R | 1.750 | 1.047 | 1.748 | 1.273 | 0.013 | 0.017 | 0.214 | 0.225 | 0.430 | 0.391 | 0.062 | 0.074 |
| FLARE | 0.720 | 0.494 | 1.346 | 1.134 | 0.009 | 0.012 | 0.171 | 0.187 | 0.251 | 0.188 | 0.048 | 0.053 |
| VGGT | 0.033 | 0.022 | 0.054 | 0.036 | 0.007 | 0.007 | 0.049 | 0.040 | 0.062 | 0.042 | 0.022 | 0.015 |
| **Ï€Â³** | **0.003** | **0.002** | **0.006** | **0.003** | **0.001** | **0.001** | **0.000** | **0.000** | **0.000** | **0.000** | **0.001** | **0.000** |

### Performance Summary
- **Camera Pose**: 55.7% better ATE on Sintel (0.074 vs VGGT 0.167)
- **Point Maps**: 30.7% better accuracy on ETH3D (0.194 vs VGGT 0.280)
- **Video Depth**: Best results across all datasets with both scale and scale&shift alignment
- **Monocular Depth**: Competitive with MoGe, best on Bonn dataset (0.044 Abs Rel)
- **Order Robustness**: Near-zero standard deviation across input permutations (Table 7)
- **Speed**: 57.4 FPS on KITTI (vs VGGT 43.2 FPS, Fast3R 65.8 FPS)
- **Efficiency**: 959M params achieving better results than models with 1.26B-5.57B params

## ðŸ’¡ Critical Analysis

### Why Ï€Â³ Works: The Power of True Symmetry

**The fundamental insight**: Visual geometry has inherent symmetry - a 3D scene looks the same regardless of viewing order. Ï€Â³ is the first to fully respect this symmetry in its architecture.

### Key Technical Innovations

1. **Complete Elimination of Positional Information**
   - No positional embeddings â†’ pure content-based processing
   - Same image produces same features regardless of position in sequence

2. **No Reference Frame Bias**
   - VGGT: First frame is special reference
   - Ï€Â³: All frames processed identically

3. **Mathematical Guarantee**
   ```
   f(Ï€(X)) = Ï€(f(X)) for any permutation Ï€
   ```
   This is not just theoretical - Table 7 proves it empirically

### Strengths
- **True Permutation Equivariance**: 10Ã— lower variance than VGGT (0.003 vs 0.033)
- **Universal Performance**: SOTA across all tasks - not specialized for one
- **Efficiency**: Smaller model (959M) outperforms larger ones (1.26B-5.57B)
- **Robustness**: Works equally well on seen and unseen datasets
- **Loss Function Design**: Achieves equivariance through architecture, not loss constraints

### Limitations
- **Computational Cost**: Global attention still has O(NÂ²) complexity for N views
- **Memory Requirements**: Processing many views simultaneously needs substantial GPU memory
- **Limited to Static Scenes**: Like most geometry methods, assumes static content

## ðŸ”— Significance and Future Impact

### Paradigm Shift in Visual Geometry
Ï€Â³ demonstrates that respecting mathematical symmetries in neural architecture design leads to both better performance and more robust systems. This principle could extend beyond visual geometry to other domains with inherent symmetries.

### Practical Applications
- **Multi-camera Systems**: Order-independent processing crucial for robotics/autonomous vehicles
- **Crowdsourced Reconstruction**: Handle unordered photo collections naturally
- **AR/VR**: Robust 3D understanding from arbitrary viewpoints

### Research Directions
- Extending permutation equivariance to dynamic scenes
- Applying similar principles to other computer vision tasks
- Exploring other symmetries in visual data

## ðŸ“š Conclusions

Ï€Â³ achieves what previous methods couldn't: true permutation equivariance in visual geometry learning. By eliminating all order-dependent components (positional embeddings, reference frames), it creates a genuinely symmetric architecture that:

1. **Performs better**: SOTA results across all benchmarks
2. **Generalizes better**: Strong zero-shot performance
3. **Behaves predictably**: Near-zero variance with input reordering

This work establishes permutation equivariance as a fundamental principle for visual geometry, opening new directions for robust and scalable 3D understanding.